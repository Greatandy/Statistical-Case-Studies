{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab8 Probabilistic-Language-Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 导入新闻数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>detail</th>\n",
       "      <th>href</th>\n",
       "      <th>origin_url</th>\n",
       "      <th>source</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>主要研究领域产业创新系统、技术创新、竞争战略、创新战略、管理整合及业务流程再造、开放式创新与...</td>\n",
       "      <td>2018/4/22</td>\n",
       "      <td>李显君：清华大学汽车工程系，汽车产业系统工程学科主任，博士生导师。主要研究领域产业创新系统、...</td>\n",
       "      <td>http://finance.sina.com.cn/china/2018-04-22/do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新浪综合</td>\n",
       "      <td>16:30:29</td>\n",
       "      <td>美国制裁中兴通讯 敲响了中国四大警钟</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>美国制裁中兴通讯可能对中国出口构成“主要风险”】瑞穗沈建光、周雪称，由于中国科技行业的迅速崛...</td>\n",
       "      <td>2018/4/19</td>\n",
       "      <td>【瑞穗证券：美国制裁\\n可能对中国出口构成“主要风险”】瑞穗沈建光、周雪称，由于中国科技...</td>\n",
       "      <td>http://finance.sina.com.cn/7x24/2018-04-19/doc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新浪财经</td>\n",
       "      <td>18:24:42</td>\n",
       "      <td>瑞穗证券：美国制裁中兴通讯可能对中国出口构成“主要风险”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国商务部表示，由于中兴通讯违反了曾与美国政府达成的和解协议，7年内禁止美国企业向中兴通讯出...</td>\n",
       "      <td>2018/4/20</td>\n",
       "      <td>当地时间16日，一记重拳向\\n砸下。美国商务部表示，由于中兴通讯违反了曾与美国政府达成的和解...</td>\n",
       "      <td>http://finance.sina.com.cn/chanjing/gsnews/201...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>央视财经</td>\n",
       "      <td>20:09:26</td>\n",
       "      <td>中兴直斥美国制裁不公平 中兴要“兴”应出什么牌？</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>彭新 4月17日，美国商务部禁止美国公司向中国主要手机生产厂商中兴通讯销售任何电子技术或通讯...</td>\n",
       "      <td>2018/4/18</td>\n",
       "      <td>彭新\\n4月17日，美国商务部禁止美国公司向中国主要手机生产厂商\\n销售任何电子技术或通讯元...</td>\n",
       "      <td>http://finance.sina.com.cn/chanjing/gsnews/201...</td>\n",
       "      <td>http://www.jiemian.com/article/2066223.html</td>\n",
       "      <td>界面</td>\n",
       "      <td>20:43:52</td>\n",
       "      <td>美国凭什么制裁中兴？ 企业遭到制裁后损失有多大？</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>富瑞表示，昂纳科技在光学元件的业务上，将会直接受惠于美国制裁中兴通讯．该行指，中兴通讯供应短...</td>\n",
       "      <td>2018/4/18</td>\n",
       "      <td>昂纳科技（00877）近期回落至5元水平，现价上升8.3%，报5.48元；成交约563万股，...</td>\n",
       "      <td>http://finance.sina.com.cn/stock/hkstock/ggscy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新浪港股</td>\n",
       "      <td>13:16:30</td>\n",
       "      <td>昂纳科技被富瑞指将受惠中兴被美国制裁 现涨8%</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract       date  \\\n",
       "0  主要研究领域产业创新系统、技术创新、竞争战略、创新战略、管理整合及业务流程再造、开放式创新与...  2018/4/22   \n",
       "1  美国制裁中兴通讯可能对中国出口构成“主要风险”】瑞穗沈建光、周雪称，由于中国科技行业的迅速崛...  2018/4/19   \n",
       "2  美国商务部表示，由于中兴通讯违反了曾与美国政府达成的和解协议，7年内禁止美国企业向中兴通讯出...  2018/4/20   \n",
       "3  彭新 4月17日，美国商务部禁止美国公司向中国主要手机生产厂商中兴通讯销售任何电子技术或通讯...  2018/4/18   \n",
       "4  富瑞表示，昂纳科技在光学元件的业务上，将会直接受惠于美国制裁中兴通讯．该行指，中兴通讯供应短...  2018/4/18   \n",
       "\n",
       "                                              detail  \\\n",
       "0  李显君：清华大学汽车工程系，汽车产业系统工程学科主任，博士生导师。主要研究领域产业创新系统、...   \n",
       "1    【瑞穗证券：美国制裁\\n可能对中国出口构成“主要风险”】瑞穗沈建光、周雪称，由于中国科技...   \n",
       "2  当地时间16日，一记重拳向\\n砸下。美国商务部表示，由于中兴通讯违反了曾与美国政府达成的和解...   \n",
       "3  彭新\\n4月17日，美国商务部禁止美国公司向中国主要手机生产厂商\\n销售任何电子技术或通讯元...   \n",
       "4  昂纳科技（00877）近期回落至5元水平，现价上升8.3%，报5.48元；成交约563万股，...   \n",
       "\n",
       "                                                href  \\\n",
       "0  http://finance.sina.com.cn/china/2018-04-22/do...   \n",
       "1  http://finance.sina.com.cn/7x24/2018-04-19/doc...   \n",
       "2  http://finance.sina.com.cn/chanjing/gsnews/201...   \n",
       "3  http://finance.sina.com.cn/chanjing/gsnews/201...   \n",
       "4  http://finance.sina.com.cn/stock/hkstock/ggscy...   \n",
       "\n",
       "                                    origin_url source      time  \\\n",
       "0                                          NaN   新浪综合  16:30:29   \n",
       "1                                          NaN   新浪财经  18:24:42   \n",
       "2                                          NaN   央视财经  20:09:26   \n",
       "3  http://www.jiemian.com/article/2066223.html     界面  20:43:52   \n",
       "4                                          NaN   新浪港股  13:16:30   \n",
       "\n",
       "                          title  period  \n",
       "0            美国制裁中兴通讯 敲响了中国四大警钟       3  \n",
       "1  瑞穗证券：美国制裁中兴通讯可能对中国出口构成“主要风险”       2  \n",
       "2      中兴直斥美国制裁不公平 中兴要“兴”应出什么牌？       3  \n",
       "3      美国凭什么制裁中兴？ 企业遭到制裁后损失有多大？       2  \n",
       "4       昂纳科技被富瑞指将受惠中兴被美国制裁 现涨8%       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "import operator\n",
    "import csv\n",
    "\n",
    "\n",
    "os.chdir(\"E:/graduate/class/Statistical Case Studies/homework7\")\n",
    "df = pd.read_csv(\"alldata.csv\",encoding = \"gb18030\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 定义文本预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(data):\n",
    "    \"\"\"获取csv文件中的新闻内容\"\"\"\n",
    "    text=data[\"detail\"]\n",
    "    text = text.dropna() \n",
    "    len(text)\n",
    "    text=[t.encode('utf-8').decode(\"utf-8\") for t in text] \n",
    "    return text\n",
    "\n",
    "def get_stop_words(file='stopWord.txt'):\n",
    "    \"\"\"去除停用词函数\"\"\"\n",
    "    file = open(file, 'rb').read().decode('utf8').split(',')\n",
    "    file = [line.strip() for line in file]\n",
    "    return set(file)\n",
    "\n",
    "\n",
    "def rm_tokens(words): \n",
    "    \"\"\"去除停用词\"\"\"\n",
    "    words_list = list(words)\n",
    "    stop_words = get_stop_words()\n",
    "    for i in range(words_list.__len__())[::-1]:\n",
    "        if words_list[i] in stop_words:                      # 去除停用词\n",
    "            words_list.pop(i)\n",
    "        elif words_list[i].isdigit():\n",
    "            words_list.pop(i)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "\n",
    "def rm_char(text):\n",
    "    \"\"\"\n",
    "    去除标点符号、特殊符号等，句子开头标记为S（for start），结尾标记为E（for end），为方便ngram模型去除与开头结尾相邻的词频\n",
    "    \"\"\"\n",
    "    text = re.sub('\\x01', '', text)                        #全角的空白符\n",
    "    text = re.sub('\\u3000', '', text) \n",
    "    text = re.sub(r\"[\\)(↓%·▲ \\s+】【]\",\"\", text.encode(\"utf-8\").decode(\"utf-8\")) \n",
    "    text = re.sub(r\"[\\n\\da-z+（）《》↗><‘’“”.-]\",\"\",text,flags=re.I)\n",
    "    text = re.sub('\\n+', \" \", text)\n",
    "    text = re.sub('[，、：。！？?；]', \" \", text)\n",
    "    text = re.sub(' +', \"E S\", text)\n",
    "    return text\n",
    "\n",
    "def convert_doc_to_wordlist(str_doc, cut_all=False):\n",
    "    sent_list = str_doc.split('\\n')\n",
    "    sent_list = map(rm_char, sent_list)                       # 去掉一些字符，例如\\u3000\n",
    "    word_2dlist = [rm_tokens(jieba.cut(part, cut_all=cut_all))\n",
    "                   for part in sent_list]                     # jieba分词\n",
    "    word_list = sum(word_2dlist, [])\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 划分不同时间段的新闻\n",
    "- 美国商务部：\n",
    "    - **2018年4月16日晚**，美国商务部发布公告称，美国政府在未来7年内禁止中兴通讯向美国企业购买敏感产品。\n",
    "\n",
    "- 中华人民共和国商务部：\n",
    "    - **2018年4月17日**，商务部新闻发言人回应指出，中方一贯要求中国企业在海外经营过程中，遵守东道国的法律政策，合法合规开展经营。中兴公司与数百家美国企业开展了广泛的贸易投资合作，为美国贡献了数以万计的就业岗位。希望美方依法依规，妥善处理，并为企业创造公正、公平、稳定的法律和政策环境。商务部将密切关注事态进展，随时准备采取必要措施，维护中国企业的合法权益。\n",
    "    - **2018年4月19日**，针对中兴被美国“封杀”的问题，商务部新闻发言人高峰在新闻发布会上表示，中方密切关注进展，随时准备采取必要措施，维护中国企业合法权益。\n",
    "\n",
    "- 中兴通讯：\n",
    "    - **2018年4月20日**，中兴通讯发布关于美国商务部激活拒绝令的声明，称在相关调查尚未结束之前，美国商务部工业与安全局执意对公司施以最严厉的制裁，对中兴通讯极不公平，“不能接受！”\n",
    "    - **2018年4月22日晚间**，中兴通讯公告，2016年4月以来，公司吸取过去在出口管制合规方面的教训，高度重视出口管制合规工作，把合规视为公司战略的基石和经营的前提及底线。美国商务部工业与安全局激活拒绝令，公司已经且正在采取措施以遵守该拒绝令。公司积极与相关方沟通以及寻求解决方案。\n",
    "\n",
    "- 划分四个时间段：\n",
    "    - 4月17日\n",
    "    - 4月18-4月19\n",
    "    - 4月20-4月23\n",
    "    - 4月24至今"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_period_i(data,clean_text,i): \n",
    "    \"\"\"#获取不同时间段的新闻文本\"\"\"\n",
    "    j=0\n",
    "    period_i=[]\n",
    "    for j in list(range(len(clean_text))):\n",
    "        if data['period'][j] == i:\n",
    "            period_i.extend(clean_text[j])\n",
    "    return period_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 获取Ngram词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top 50 wordcount of 2 gram model of period_1 is:\n",
      " [(('美国', '企业'), 128), (('中', '美'), 87), (('中国', '企业'), 72), (('违反', '美国'), 57), (('禁运', '事件'), 56), (('出口', '管制'), 48), (('贸易', '摩擦'), 47), (('自主', '可控'), 46), (('名', '员工'), 46), (('财经', '记者'), 46), (('第一', '财经'), 46), (('记者', '表示'), 46), (('拒绝', '令'), 44), (('高级', '雇员'), 44), (('达成', '和解'), 42), (('集成电路', '产业'), 42), (('中国', '商务部'), 40), (('禁止', '美国'), 40), (('和解', '协议'), 40), (('激活', '拒绝'), 40), (('美国商务部', '公司'), 38), (('名', '高级'), 36), (('美国', '供应商'), 36), (('出口', '限制'), 34), (('中兴', '美国'), 34), (('基带', '芯片'), 34), (('解雇', '名'), 34), (('公司', '激活'), 34), (('可能', '产生'), 32), (('市场', '约'), 32), (('约', '市场份额'), 32), (('积极', '沟通'), 32), (('设备', '市场'), 32), (('国产', '芯片'), 32), (('中国电信', '设备'), 31), (('产生', '影响'), 30), (('评估', '事件'), 30), (('正在', '全面'), 30), (('全面', '评估'), 30), (('通信', '设备'), 30), (('公司', '可能'), 30), (('事件', '公司'), 30), (('公司', '正在'), 30), (('亿美元', '罚款'), 30), (('国家', '安全'), 30), (('沟通', '应对'), 28), (('特朗普', '政府'), 28), (('工业', '安全局'), 28), (('方面', '积极'), 28), (('美国', '制裁'), 28)] /n\n",
      "the top 50 wordcount of 2 gram model of period_4 is:\n",
      " [(('贸易', '摩擦'), 60), (('研究', '中心'), 49), (('银河', '期货'), 44), (('资讯', '银河'), 42), (('中', '美'), 42), (('期货', '研究'), 42), (('—', '—'), 42), (('芯片', '行业'), 40), (('集成电路', '产业'), 40), (('全球', '最大'), 38), (('资料', '来源'), 38), (('中国', '制造'), 37), (('中国', '企业'), 37), (('国产', '芯片'), 37), (('中国', '手机'), 36), (('中美', '贸易'), 36), (('数据', '显示'), 34), (('出口', '管制'), 32), (('证券', '认为'), 31), (('市值', '亿元'), 30), (('同比', '增长'), 30), (('美元', '指数'), 30), (('中美', '贸易战'), 30), (('美国', '制裁'), 29), (('芯片', '企业'), 29), (('经济', '增长'), 29), (('避险', '情绪'), 28), (('中兴', '事件'), 27), (('高质量', '发展'), 26), (('芯片', '研发'), 26), (('半导体', '行业'), 25), (('美国', '来说'), 24), (('手机', '品牌'), 24), (('美国', '总统'), 24), (('美国', '企业'), 24), (('经济', '发展'), 24), (('国内', '芯片'), 24), (('证券', '指出'), 22), (('中国', '经济'), 22), (('中国', '进行'), 22), (('中国', '人'), 22), (('自主', '创新'), 22), (('调查', '华为'), 22), (('经济', '工作'), 22), (('工作', '会议'), 22), (('产业', '发展'), 22), (('美债', '收益率'), 22), (('违反', '美国'), 21), (('认为', '美国'), 21), (('共享', '单车'), 21)] /n\n",
      "the top 50 wordcount of 2 gram model of all_period is:\n",
      " [(('美国', '企业'), 369), (('中国', '企业'), 315), (('贸易', '摩擦'), 306), (('中', '美'), 297), (('美国', '制裁'), 230), (('出口', '管制'), 212), (('国产', '芯片'), 197), (('中美', '贸易'), 176), (('集成电路', '产业'), 174), (('违反', '美国'), 169), (('中国', '商务部'), 154), (('数据', '显示'), 151), (('中国', '制造'), 148), (('第一', '财经'), 142), (('达成', '和解'), 136), (('国家', '安全'), 133), (('原产', '美国'), 124), (('中兴', '事件'), 124), (('财经', '记者'), 124), (('美国', '供应商'), 123), (('同比', '增长'), 120), (('中兴', '美国'), 118), (('拒绝', '令'), 116), (('出口', '禁令'), 115), (('记者', '表示'), 115), (('中美', '贸易战'), 113), (('通信', '设备'), 112), (('半导体', '产业'), 112), (('产业', '发展'), 109), (('美国', '出口'), 108), (('禁止', '美国'), 107), (('美国', '中国'), 106), (('美', '贸易'), 106), (('中国', '芯片'), 103), (('美国', '贸易'), 102), (('美国商务部', '宣布'), 99), (('．', '通'), 98), (('—', '—'), 98), (('半导体', '行业'), 97), (('国产', '替代'), 96), (('特朗普', '政府'), 96), (('情况', '下'), 95), (('工业', '安全局'), 95), (('基金', '公司'), 94), (('软件', '技术'), 94), (('和解', '协议'), 93), (('密切', '关注'), 92), (('美国商务部', '工业'), 91), (('芯片', '行业'), 91), (('自主', '可控'), 90)] /n\n"
     ]
    }
   ],
   "source": [
    "def getNgramDict(word_seg_list,n=2):\n",
    "    \"\"\"\n",
    "    获得不同语料库的Ngram词频统计\n",
    "    \"\"\"\n",
    "    rm_space = [w for w in word_seg_list if w not in [\"\",\" \"]]\n",
    "    \n",
    "    analyzer2 = ngrams(rm_space,n)\n",
    "    Ngram_dict = Counter(analyzer2)#字典的键值是元组\n",
    "\n",
    "    #删除包含开头结尾的二元词频统计\n",
    "    key_list=[]\n",
    "    for key in Ngram_dict.keys():\n",
    "        if \"E\" in key or \"S\" in key:\n",
    "            key_list.append(key)\n",
    "    \n",
    "    for key in key_list:    \n",
    "        del Ngram_dict[key]\n",
    "\n",
    "    return Ngram_dict\n",
    "\n",
    "\n",
    "clean_text=[convert_doc_to_wordlist(line) for line in get_text(df)]\n",
    "period_1 = get_period_i(df,clean_text,1)\n",
    "period_4 = get_period_i(df,clean_text,4)\n",
    "all_period = [w for doc in clean_text for w in doc]\n",
    "\n",
    "\n",
    "period_1_dict = getNgramDict(period_1,n=2)\n",
    "period_4_dict = getNgramDict(period_4,n=2)\n",
    "all_period_dict = getNgramDict(all_period,n=2)\n",
    "\n",
    "print_n=50\n",
    "n =2\n",
    "sortedNGrams = sorted(period_1_dict.items(), key = operator.itemgetter(1), reverse=True) #=True 降序排列\n",
    "print(\"the top %d wordcount of %d gram model of period_1 is:\\n\" %(print_n,n),sortedNGrams[:print_n],\"/n\")\n",
    "sortedNGrams = sorted(period_4_dict.items(), key = operator.itemgetter(1), reverse=True) #=True 降序排列\n",
    "print(\"the top %d wordcount of %d gram model of period_4 is:\\n\" %(print_n,n),sortedNGrams[:print_n],\"/n\")\n",
    "sortedNGrams = sorted(all_period_dict.items(), key = operator.itemgetter(1), reverse=True) #=True 降序排列\n",
    "print(\"the top %d wordcount of %d gram model of all_period is:\\n\" %(print_n,n),sortedNGrams[:print_n],\"/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 基于Ngram模型的文本分类器\n",
    "\n",
    "根据每个类别的语料库训练各自的语言模型，实质上就是每一个类别都有一个概率分布，当新来一个文本的时候，只要根据各自的语言模型，计算出每个语言模型下这篇文本的发生概率，文本在哪个模型的概率大，这篇文本就属于哪个类别了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 计算二元词频概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the corresponding probability for each entry in the indonesian LM  \n",
    "sum_indo=0\n",
    "for (k,v) in period_1_dict.items():  \n",
    "    sum_indo += v  \n",
    "for (k,v) in period_1_dict.items():  \n",
    "    period_1_dict[k] = v/sum_indo  \n",
    "sum_indo=0\n",
    "for (k,v) in period_4_dict.items():  \n",
    "    sum_indo += v  \n",
    "for (k,v) in period_4_dict.items():  \n",
    "    period_4_dict[k] = v/sum_indo  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 平滑化\n",
    "\n",
    "period1 period4 的语料库中未出现过，而总语料库中出现过的词，赋值1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add one smoothing  \n",
    "period_1_dict_s = period_1_dict.copy()\n",
    "period_4_dict_s = period_4_dict.copy()\n",
    "for key in  all_period_dict:\n",
    "    if key not in period_1_dict_s:\n",
    "        period_1_dict_s[key]=0.00000001\n",
    "    if key not in period_4_dict_s:\n",
    "        period_4_dict_s[key]=0.00000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看预测结果\n",
    "\n",
    "使用period2 period3的新闻作为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrediction(clean_text_period):\n",
    "    predict_period = []\n",
    "    for doc in clean_text_period:\n",
    "        doc_dict = getNgramDict(doc,n=2)\n",
    "        period_1_pro = 1  \n",
    "        period_4_pro = 1  \n",
    "        for word in doc_dict:  \n",
    "                #check if the word is in the entry, if it is, calculate the probability  \n",
    "            if word in all_period_dict:  \n",
    "                period_1_pro = period_1_pro + np.log(period_1_dict_s[word])  #乘多了就变0了\n",
    "                period_4_pro = period_4_pro + np.log(period_4_dict_s[word])\n",
    "                    \n",
    "        if period_1_pro > period_4_pro: \n",
    "            predict_period.append(1)\n",
    "#            print(\"the article is from period_1\")\n",
    "        elif period_1_pro < period_4_pro: \n",
    "            predict_period.append(4)\n",
    "#            print(\"the article is from period_4\")   \n",
    "        else:\n",
    "            predict_period.append(0)\n",
    "#            print(\"can't predict the article's period\")   \n",
    "    return predict_period\n",
    "\n",
    "\n",
    "\n",
    "#获取时期1 和4的新闻\n",
    "df2 = df[df['period'].values==2].copy()\n",
    "df3 = df[df['period'].values==3].copy()\n",
    "clean_text2=[convert_doc_to_wordlist(line) for line in get_text( df2)]\n",
    "clean_text3=[convert_doc_to_wordlist(line) for line in get_text( df3)]    \n",
    "\n",
    "#不同时间段的新闻数量\n",
    "df[\"period\"].value_counts()\n",
    "\n",
    "#查看预测结果\n",
    "def seePrediction(data,i):\n",
    "    df_i = data[data['period'].values==i].copy()\n",
    "    clean_text_i=[convert_doc_to_wordlist(line) for line in get_text(df_i)]\n",
    "    p_i = getPrediction(clean_text_i)\n",
    "    df_i[\"pre\"] = p_i\n",
    "    date_pre = []\n",
    "    for j in df_i.index:\n",
    "        date_pre.append((df_i.ix[j,\"date\"],df_i.ix[j,\"pre\"]))\n",
    "    return Counter(date_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\situ\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction of period_2:\n",
      " Counter({('2018/4/18', 1): 47, ('2018/4/19', 1): 30, ('2018/4/19', 4): 30, ('2018/4/18', 4): 17, ('2018/4/18', 0): 1}) \n",
      "\n",
      "the prediction of period_2:\n",
      " Counter({('2018/4/22', 4): 11, ('2018/4/23', 4): 7, ('2018/4/20', 4): 7, ('2018/4/20', 1): 4, ('2018/4/21', 4): 3, ('2018/4/23', 1): 2, ('2018/4/22', 1): 2, ('2018/4/21', 1): 1, ('2018/4/22', 0): 1}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"the prediction of period_2:\\n\",seePrediction(df,2),\"\\n\")\n",
    "print(\"the prediction of period_2:\\n\",seePrediction(df,3),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 提取所有新闻文本并分词\n",
    "\n",
    "无需去除标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李显君 ： 清华大学 汽车 工程系 ， 汽车产业 系统工程 学科 主任 ， 博士生 导师 。 主要 研究 领域 产业 创新 系统 、 技术创新 、 竞争 战略 、 创新 战略 、 管理 整合 及 业务流程 再造 、 开放式 创新 与 经营 模式 。   当 美国 对 叙利亚 狂 轰乱 炸 并 追加 制裁 俄罗斯 之际 ， 很多 国人 ， 包括 一些 号称 著名 战略家 以为 它 对 中国 的 贸易战 会 偃旗息鼓 ， 至少 现阶段 不会 再起 风浪 。 然而 ， 出乎 太 多 善良 国人 意料 的 是 ， 北京 时间 4 月 16 日 ， 特朗普 政府 突然 对 我国 中兴通讯 公司 实施 制裁 ， 定点 打击 。   担忧 与 绝望 、 愤怒 与 慷慨 、 决心 与 斗志 ， 瞬间 蔓延 于线 上线 下 。   而 我 要说 的 ： 感谢 特朗普 及 美国政府 这一 制裁 ！ 因为 ：   它 让 中国 彻底 明白 了 美国 贸易战 的 意图 ， 否则 我们 还 陶醉 在 对 美 大豆 等 高明 的 “ 七寸 反击 ” 中 ；   它 让 中国 彻底 清醒 冷静 对待 自身 的 实力 ， 否则 我们 还 沉醉于 “ 经济 奇迹 ” 和 “ 新 四大发明 ” 而 不能自拔 。   它 让 中国 决心 突破 制约 产业 的 核心 瓶颈 ， 否则 我们 会 在 芯片 等 核心技术 的 依赖 中 逐渐 丧失 竞争 资本 。   具体 而言 ， 美国 制裁 中兴通讯 ， 敲响 了 制约 中国崛起 的 四座 丧钟 ！ ? ?   很多 智库 和 战略 研究者 ， 根本 没有 预测 到 这场 来时 凶猛 的 贸易战 ， 时至今日 仍 有人 天真 地 认为 特朗普 很快 就 会 回到 谈判桌上 。   关于 中 美 贸易战 的 原因 ， 战略 短视 更是 无以复加 ， 主流 观点 是 ： 特朗普 为了 中期 选举 和 商人 敲诈 的 本性 。 这种 观点 对 我国 制定 长远 且 有 针对 的 对策 危害 至极 ， 比如 认为 此次 贸易战 原因 是 为了 中期 选取 ， 便 导致 了 我们 的 反击 重点 就是 美国 的 大豆 ， 因为 豆农是 特朗普 重要 的 票 仓 来源 。 但是 美国 社会 根本 不是 由 农民 左右 的 ， 而是 华尔街 ， 是 金融寡头 ！ 因此 ，\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec  \n",
    "import logging  \n",
    "\n",
    "#把所有新闻文档放一起，分好词\n",
    "os.chdir(\"E:/graduate/class/Statistical Case Studies/homework7\")\n",
    "def cutWord(line):\n",
    "    line = re.sub(' +', \" \",line)\n",
    "    line = re.sub('\\n+', \" \",line)\n",
    "    line = re.sub('\\x01', '',line)                        #全角的空白符\n",
    "    line = re.sub('\\u3000', '', line)   \n",
    "    seg_list = jieba.cut(line, cut_all=False)  \n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "cut_text=[cutWord(line) for line in get_text(df)]\n",
    "print(\" \".join(cut_text)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 写入txt文件\n",
    "\n",
    "word2vec要求导入utf-8的文本文件作为语料库，最好能达到1G以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_cut_text = \" \".join(cut_text)\n",
    "f2 =open(\"fenci_result.txt\", 'a')  \n",
    "f2.write(\" \".join(cut_text))  \n",
    "f2.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 训练word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-14 00:29:14,625:INFO: collecting all words and their counts\n",
      "2018-05-14 00:29:14,631:INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-14 00:29:15,245:INFO: collected 22726 word types from a corpus of 1016251 raw words and 102 sentences\n",
      "2018-05-14 00:29:15,250:INFO: Loading a fresh vocabulary\n",
      "2018-05-14 00:29:15,300:INFO: min_count=5 retains 9653 unique words (42% of original 22726, drops 13073)\n",
      "2018-05-14 00:29:15,300:INFO: min_count=5 leaves 981868 word corpus (96% of original 1016251, drops 34383)\n",
      "2018-05-14 00:29:15,366:INFO: deleting the raw counts dictionary of 22726 items\n",
      "2018-05-14 00:29:15,367:INFO: sample=0.001 downsamples 25 most-common words\n",
      "2018-05-14 00:29:15,371:INFO: downsampling leaves estimated 626242 word corpus (63.8% of prior 981868)\n",
      "2018-05-14 00:29:15,422:INFO: estimated required memory for 9653 words and 200 dimensions: 20271300 bytes\n",
      "2018-05-14 00:29:15,426:INFO: resetting layer weights\n",
      "2018-05-14 00:29:15,689:INFO: training model with 3 workers on 9653 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-14 00:29:16,702:INFO: EPOCH 1 - PROGRESS: at 67.65% examples, 486515 words/s, in_qsize 5, out_qsize 1\n",
      "2018-05-14 00:29:16,968:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-14 00:29:16,985:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-14 00:29:16,985:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-14 00:29:16,991:INFO: EPOCH - 1 : training on 1016251 raw words (626192 effective words) took 1.3s, 484450 effective words/s\n",
      "2018-05-14 00:29:17,989:INFO: EPOCH 2 - PROGRESS: at 75.49% examples, 519834 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-14 00:29:18,187:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-14 00:29:18,209:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-14 00:29:18,212:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-14 00:29:18,214:INFO: EPOCH - 2 : training on 1016251 raw words (626161 effective words) took 1.2s, 513710 effective words/s\n",
      "2018-05-14 00:29:19,224:INFO: EPOCH 3 - PROGRESS: at 77.45% examples, 527671 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-14 00:29:19,392:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-14 00:29:19,412:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-14 00:29:19,414:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-14 00:29:19,418:INFO: EPOCH - 3 : training on 1016251 raw words (626313 effective words) took 1.2s, 522444 effective words/s\n",
      "2018-05-14 00:29:20,423:INFO: EPOCH 4 - PROGRESS: at 66.67% examples, 483389 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-14 00:29:20,721:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-14 00:29:20,731:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-14 00:29:20,740:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-14 00:29:20,742:INFO: EPOCH - 4 : training on 1016251 raw words (626663 effective words) took 1.3s, 475050 effective words/s\n",
      "2018-05-14 00:29:21,749:INFO: EPOCH 5 - PROGRESS: at 77.45% examples, 529209 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-14 00:29:21,946:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-14 00:29:21,948:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-14 00:29:21,956:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-14 00:29:21,957:INFO: EPOCH - 5 : training on 1016251 raw words (626174 effective words) took 1.2s, 517152 effective words/s\n",
      "2018-05-14 00:29:21,959:INFO: training on a 5081255 raw words (3131503 effective words) took 6.3s, 499966 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=9653, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s:%(levelname)s: %(message)s', level=logging.INFO)  \n",
    "sentences =word2vec.Text8Corpus(u\"fenci_result.txt\")  # 加载语料  \n",
    "#Initialize a model\n",
    "model =word2vec.Word2Vec(sentences, size=200)  #训练skip-gram模型，默认window=5  \n",
    "print(model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 查看词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.29079044e-01  -4.84883726e-01  -5.75685203e-01   4.33477551e-01\n",
      "   4.81799960e-01  -5.07514298e-01   2.71963864e-03  -2.32927591e-01\n",
      "   8.69941175e-01  -9.04398382e-01   3.59901547e-01   6.64961562e-02\n",
      "   2.20925510e-01   1.21856458e-01   8.74979973e-01   5.11522293e-01\n",
      "   8.02101493e-01  -2.76886821e-01   4.02252018e-01   1.68653533e-01\n",
      "   5.66319287e-01   2.50396818e-01   4.54001367e-01  -6.03304148e-01\n",
      "   3.81084174e-01  -2.37570018e-01   9.36034262e-01   5.52148938e-01\n",
      "   3.57077986e-01   3.73060614e-01  -1.01284134e+00   5.82265377e-01\n",
      "   4.00560915e-01   5.05854130e-01   4.84963469e-02   2.16357708e-01\n",
      "  -3.99398804e-01   8.94760668e-01   1.06859155e-01   1.11635756e+00\n",
      "  -8.23819518e-01  -3.96804154e-01  -2.30362236e-01  -4.82189894e-01\n",
      "  -3.77694726e-01  -6.95044249e-02   9.85528901e-02  -6.19355202e-01\n",
      "   1.92789122e-01   3.82011384e-01  -4.68891025e-01  -7.50730574e-01\n",
      "   4.42582369e-01   2.69690275e-01   1.01212576e-01   1.71006233e-01\n",
      "  -5.47219574e-01   1.19261837e+00   6.46237791e-01   9.65291485e-02\n",
      "   5.73532939e-01   1.18172359e+00  -1.52051225e-01   8.83690655e-01\n",
      "   1.46083355e+00  -2.06772342e-01   3.35356027e-01   4.09277618e-01\n",
      "   1.47840083e-01  -1.48921143e-02   7.20948935e-01   7.90126681e-01\n",
      "   1.02363396e+00   4.95381355e-02   8.12551439e-01   7.35886097e-01\n",
      "   8.39633822e-01   3.73193771e-02   9.08718288e-01  -3.89890999e-01\n",
      "   7.98440456e-01   1.17803834e-01   9.00675118e-01   8.76559436e-01\n",
      "  -5.87955117e-01   1.60570994e-01   1.08735733e-01   3.53182852e-01\n",
      "  -1.43683434e+00  -6.89835787e-01   1.24772024e+00   5.53873301e-01\n",
      "   2.69182533e-01  -7.96763837e-01   1.25409520e+00  -1.16261229e-01\n",
      "   1.27839124e+00   6.45714521e-01  -1.14666033e+00   3.38005930e-01\n",
      "   6.22092903e-01  -3.71749164e-04   1.76699430e-01  -6.97272047e-02\n",
      "   2.27789238e-01  -8.24571967e-01   1.17328358e+00   1.79530844e-01\n",
      "  -2.89137304e-01  -1.15160429e+00   1.00400817e+00   7.39187539e-01\n",
      "   1.24270618e+00   5.13900161e-01   3.56858701e-01   1.12683213e+00\n",
      "  -6.08587980e-01   3.35509330e-01  -4.12521571e-01  -3.60178173e-01\n",
      "   2.32612461e-01  -3.74186970e-02   2.28676453e-01   5.72760105e-02\n",
      "   5.65170527e-01  -4.79673237e-01   9.77046609e-01   6.75317585e-01\n",
      "   3.74187171e-01   9.10254121e-01   5.56963921e-01  -4.74828154e-01\n",
      "  -5.86485207e-01  -8.97616595e-02   9.79490280e-01  -1.25257447e-01\n",
      "   8.25464070e-01   7.33678758e-01   8.43454182e-01   1.36239707e+00\n",
      "   5.47924280e-01   4.75541145e-01  -5.62003925e-02  -9.07813832e-02\n",
      "   3.31798166e-01  -3.53649884e-01  -1.18505634e-01   6.14663005e-01\n",
      "   4.05683994e-01  -3.72255027e-01  -3.98737609e-01   3.27758789e-02\n",
      "  -1.42511517e-01   2.42015705e-01  -5.12934566e-01   7.14756027e-02\n",
      "  -1.08849692e+00  -3.99785310e-01  -3.38482857e-01  -1.71197206e-01\n",
      "   1.33230315e-05   8.49252522e-01  -1.26921773e-01  -4.49102759e-01\n",
      "  -1.04886495e-01  -5.27440310e-02   1.28548563e-01  -7.56439984e-01\n",
      "  -6.84086740e-01  -4.03364807e-01  -3.99800360e-01   4.89785075e-01\n",
      "   2.41234154e-01   2.22748175e-01   3.95062000e-01  -7.91335583e-01\n",
      "   8.89670789e-01   7.00181425e-01  -9.15142953e-01  -1.31365371e+00\n",
      "   2.23090962e-01   2.53765122e-03  -1.41231823e+00   9.01865780e-01\n",
      "  -1.53069511e-01   6.53726637e-01   1.01392366e-01   3.38033348e-01\n",
      "  -1.26068503e-01  -2.33790413e-01  -1.53319448e-01   1.86306655e-01\n",
      "   2.13139758e-01   4.56539631e-01  -3.78986269e-01  -5.48656404e-01\n",
      "  -2.53121167e-01  -7.64625311e-01  -3.03151280e-01   1.65694818e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy vector of a word\n",
    "print(model.wv['中美'])\n",
    "len(model.wv['中美'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 语义相关性的探索发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\situ\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\situ\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "C:\\Users\\situ\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "2018-05-14 00:33:36,801:INFO: saving Word2Vec object under 中兴事件.model, separately None\n",
      "2018-05-14 00:33:36,811:INFO: not storing attribute vectors_norm\n",
      "2018-05-14 00:33:36,812:INFO: not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【制裁】和【禁令】的相似度为0.897615\n",
      "-----------------------------------------------------\n",
      "\n",
      "和【中兴】最相关的词有：\n",
      "\n",
      "中兴通讯 0.8524785041809082\n",
      "法巴 0.7526480555534363\n",
      "美国政府 0.7278330326080322\n",
      "此次 0.714938223361969\n",
      "伊朗 0.7128933072090149\n",
      "华为 0.7055028676986694\n",
      "俄罗斯 0.6956233382225037\n",
      "俄 0.6800441145896912\n",
      "其 0.6674633026123047\n",
      "违反 0.6668750047683716\n",
      "本次 0.6621092557907104\n",
      "中兴公司 0.6578356027603149\n",
      "禁运 0.6504078507423401\n",
      "暂缓 0.6499906778335571\n",
      "内容 0.6447697877883911\n",
      "履行 0.6447207927703857\n",
      "减轻 0.639124870300293\n",
      "罚金 0.637557864189148\n",
      "认定 0.6357647776603699\n",
      "禁售 0.6334564685821533\n",
      "-----------------------------------------------------\n",
      "\n",
      "【制裁 禁令 封杀 打击 美国】中不合群的词： 美国\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-14 00:33:37,248:INFO: saved 中兴事件.model\n"
     ]
    }
   ],
   "source": [
    "# 计算两个词的相似度/相关程度  \n",
    "words = [\"制裁\",\"禁令\"]\n",
    "#words = [\"中兴\",\"华为\"]\n",
    "try:  \n",
    "    y1 = model.similarity(words[0],words[1])  \n",
    "except KeyError:  \n",
    "    y1 = 0  \n",
    "print(\"【%s】和【%s】的相似度为%f\" %(words[0],words[1],y1))  \n",
    "print(\"-----------------------------------------------------\\n\")  \n",
    "#  \n",
    "# 计算某个词的余弦相关词列表  \n",
    "#word = \"制裁\"\n",
    "word = \"中兴\"\n",
    "y2 = model.most_similar(word, topn=20)  # 20个最相关的  \n",
    "print(\"和【%s】最相关的词有：\\n\" %word)  \n",
    "for item in y2:  \n",
    "    print(item[0], item[1])  \n",
    "print(\"-----------------------------------------------------\\n\")  \n",
    "\n",
    "   \n",
    "# 寻找不合群的词 \n",
    "words = u\"制裁 禁令 封杀 打击 美国\"\n",
    "y4 =model.doesnt_match(words.split())  \n",
    "print(u\"【%s】中不合群的词：\" % words,y4)  \n",
    "print(\"-----------------------------------------------------\\n\")  \n",
    "   \n",
    "# 保存模型，以便重用  \n",
    "model.save(u\"中兴事件.model\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
